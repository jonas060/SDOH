{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import outcomes dataset\n",
    "places_500_cities = pd.read_csv(\"Data/500_Cities__Local_Data_for_Better_Health__2019_release_20240314.zip\", dtype={'TractFIPS': str})\n",
    "\n",
    "# Keep census tract level samples only\n",
    "ct_500_cities = places_500_cities[places_500_cities['GeographicLevel'] == 'Census Tract']\n",
    "\n",
    "# Drop columns not useful\n",
    "columns_to_remove = ['StateDesc', 'CityName', 'GeographicLevel', 'DataSource', 'Category', 'UniqueID', 'Measure', 'Data_Value_Unit', 'DataValueTypeID', 'Data_Value_Type', 'Low_Confidence_Limit', 'High_Confidence_Limit', 'Data_Value_Footnote_Symbol', 'Data_Value_Footnote', 'PopulationCount', 'GeoLocation', 'CategoryID', 'Short_Question_Text']\n",
    "ct_500_cities = ct_500_cities.drop(columns=columns_to_remove)\n",
    "\n",
    "## These are positive measurements, while the rest in the data are negative. We want high percentages to be bad. So for positive, I am taking the complement.\n",
    "ct_500_cities.loc[ct_500_cities['MeasureId'].isin(['DENTAL', 'COLON_SCREEN', 'COREW', 'COREM', 'BPMED', 'CHOLSCREEN', 'CHECKUP', 'PAPTEST', 'MAMMOUSE']), 'Data_Value'] = 100 - ct_500_cities.loc[ct_500_cities['MeasureId'].isin(['DENTAL', 'COLON_SCREEN', 'COREW', 'BPMED', 'CHOLSCREEN', 'CHECKUP', 'PAPTEST', 'MAMMOUSE']), 'Data_Value']\n",
    "# ct_500_cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# HPSAs could sometimes be county-wide or county subdivisions. This data will work as a crosswalk to get the census tracts within those areas.\n",
    "csd_to_ct_2010 = pd.read_csv(\"Data/CSD_to_CT_2010.csv\", dtype=str)\n",
    "# Format fields for join with census tracts in the other datasets.\n",
    "csd_to_ct_2010['csd'] = csd_to_ct_2010['county'] + csd_to_ct_2010['cousubfp']\n",
    "csd_to_ct_2010['tract'] = csd_to_ct_2010['tract'].apply(lambda x: x + '00' if '.' not in x else x)\n",
    "csd_to_ct_2010['ct'] = csd_to_ct_2010['county'] + csd_to_ct_2010['tract'].str.replace(\".\", \"\")\n",
    "\n",
    "# Health Professional Shortage Areas - Primary Care & Dental Health\n",
    "hpsa_pc_full = pd.read_csv(\"Data\\BCD_HPSA_FCT_DET_PC.csv\")\n",
    "hpsa_dh_full = pd.read_csv(\"Data\\BCD_HPSA_FCT_DET_DH.csv\")\n",
    "hpsa_pc_dh_full = pd.merge(hpsa_pc_full, hpsa_dh_full, how='outer')\n",
    "\n",
    "# Data files shows historical dates of when an area has a change in HPSA status.\n",
    "# Picking all areas that were listed as an HSPA on 12/31/2017 to avoid duplications.\n",
    "hpsa_pc_dh = hpsa_pc_dh_full[(pd.to_datetime(hpsa_pc_dh_full['HPSA Designation Date']) <= '2017-12-31')]\n",
    "hpsa_pc_dh = hpsa_pc_dh[(pd.to_datetime(hpsa_pc_dh['Withdrawn Date']) > '2017-12-31') | hpsa_pc_dh['Withdrawn Date'].isna()]\n",
    "\n",
    "# Not specific locations (eg correctional facilities)\n",
    "hpsa_pc_dh = hpsa_pc_dh[hpsa_pc_dh['HPSA Geography Identification Number'] != 'POINT']\n",
    "\n",
    "# Only when everybody in a specifc area has a shortage, not specific populations within an area (eg migrant workers)\n",
    "hpsa_pc_dh = hpsa_pc_dh[hpsa_pc_dh['Designation Type'] == 'Geographic HPSA'] \n",
    "\n",
    "# Remove columns - some might be useful but will want to get the demographic info from another source that has the info for all census tracts.\n",
    "hpsa_pc_dh = hpsa_pc_dh.loc[:, ['HPSA Discipline Class', 'HPSA Score', 'PC MCTA Score', 'HPSA Geography Identification Number', 'HPSA Component Type Code', 'HPSA Designation Date']]\n",
    "\n",
    "# Sorting data by designation date. If there are duplicates, keeping the older record\n",
    "hpsa_pc_dh = hpsa_pc_dh.sort_values(by='HPSA Designation Date')\n",
    "hpsa_pc_dh = hpsa_pc_dh.drop_duplicates(subset='HPSA Geography Identification Number', keep='first')\n",
    "\n",
    "# Creating the specific columns for primary care and dental health\n",
    "hpsa_pc_dh['PC_HPSA_Score'] = np.where(hpsa_pc_dh['HPSA Discipline Class'] == 'Primary Care', hpsa_pc_dh['HPSA Score'], np.nan)\n",
    "hpsa_pc_dh['DH_HPSA_Score'] = np.where(hpsa_pc_dh['HPSA Discipline Class'] == 'Dental Health', hpsa_pc_dh['HPSA Score'], np.nan)\n",
    "\n",
    "# Remove columns that aren't needed anymore\n",
    "hpsa_pc_dh = hpsa_pc_dh.drop(columns=['HPSA Designation Date', 'HPSA Score', 'HPSA Discipline Class'])\n",
    "\n",
    "# (1) Load ACS data (2) Drop the column with the row index (3) For all features, convert non-numeric values to NaN values\n",
    "acs_dp05 = pd.read_csv(\"Data\\ACSDP5Y2017.DP05-Data_CLEAN.csv\", dtype={\"Geography\": str})\n",
    "# Remove duplicate column\n",
    "acs_dp05 = acs_dp05.drop(columns=['Unnamed: 0', 'Percent Estimate!!RACE!!Total population!!One race.1'])\n",
    "\n",
    "for column in acs_dp05.columns:\n",
    "  if column != 'Geography':\n",
    "    acs_dp05[column] = pd.to_numeric(acs_dp05[column], errors='coerce')\n",
    "\n",
    "acs_s1810 = pd.read_csv(\"Data\\ACSDP5Y2017.S1810-Data_CLEAN.csv\", dtype={\"Geography\": str})\n",
    "\n",
    "for column in acs_s1810.columns:\n",
    "  if column.startswith('Estimate!!Percent'):\n",
    "      acs_s1810[column] = pd.to_numeric(acs_s1810[column], errors='coerce')\n",
    "\n",
    "acs_s1810 = acs_s1810.drop(columns='Unnamed: 0')\n",
    "\n",
    "acs_dp03 = pd.read_csv(\"Data\\ACSDP5Y2017.DP03-Data_CLEAN.csv\", dtype={\"Geography\": str})\n",
    "acs_dp03 = acs_dp03.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "for column in acs_dp03.columns:\n",
    "  if column != 'Geography':\n",
    "    acs_dp03[column] = pd.to_numeric(acs_dp03[column], errors='coerce')\n",
    "\n",
    "acs_dp02 = pd.read_csv(\"Data\\ACSDP5Y2017.DP02-Data_CLEAN.csv\", dtype={\"Geography\": str})\n",
    "acs_dp02 = acs_dp02.drop(columns=['Unnamed: 0'])\n",
    "# Getting disability info from another dataset\n",
    "acs_dp02 = acs_dp02.drop(columns=[col for col in acs_dp02.columns if col.startswith('Percent Estimate!!DISABILITY STATUS OF THE CIVILIAN NONINSTITUTIONALIZED POPULATION!!')]) \n",
    "\n",
    "for column in acs_dp02.columns:\n",
    "  if column != 'Geography':\n",
    "    acs_dp02[column] = pd.to_numeric(acs_dp02[column], errors='coerce')\n",
    "\n",
    "# For fields without a complement, add a complement\n",
    "acs_dp02['Percent Estimate!!COMPUTERS AND INTERNET USE!!Total households!!Without a computer'] = 100 - acs_dp02['Percent Estimate!!COMPUTERS AND INTERNET USE!!Total households!!With a computer']\n",
    "acs_dp02['Percent Estimate!!COMPUTERS AND INTERNET USE!!Total households!!Without a broadband Internet subscription'] = 100 - acs_dp02['Percent Estimate!!COMPUTERS AND INTERNET USE!!Total households!!With a broadband Internet subscription'] # Adding complement\n",
    "acs_dp02[\"Percent Estimate!!EDUCATIONAL ATTAINMENT!!Population 25 years and over!!Percent less than bachelor's degree\"] = 100 - acs_dp02[\"Percent Estimate!!EDUCATIONAL ATTAINMENT!!Population 25 years and over!!Percent bachelor's degree or higher\"] # Adding complement\n",
    "acs_dp02[\"Percent Estimate!!EDUCATIONAL ATTAINMENT!!Population 25 years and over!!Percent less than high school graduate\"] = 100 - acs_dp02[\"Percent Estimate!!EDUCATIONAL ATTAINMENT!!Population 25 years and over!!Percent high school graduate or higher\"] # Adding complement\n",
    "\n",
    "acs_dp04 = pd.read_csv(\"Data\\ACSDP5Y2017.DP04-Data_CLEAN.csv\", dtype={\"Geography\": str})\n",
    "acs_dp04 = acs_dp04.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "for column in acs_dp04.columns:\n",
    "  if column != 'Geography':\n",
    "    acs_dp04[column] = pd.to_numeric(acs_dp04[column], errors='coerce')\n",
    "\n",
    "\n",
    "# Separate HPSA information based on how the geographic area is defined (counties/county subdivisions/census tracts)\n",
    "hpsa_cty = hpsa_pc_dh[hpsa_pc_dh['HPSA Component Type Code'] == 'SCTY'].drop(columns=['HPSA Component Type Code'])\n",
    "hpsa_csd = hpsa_pc_dh[hpsa_pc_dh['HPSA Component Type Code'] == 'CSD'].drop(columns=['HPSA Component Type Code'])\n",
    "hpsa_ct = hpsa_pc_dh[hpsa_pc_dh['HPSA Component Type Code'] == 'CT'].drop(columns=['HPSA Component Type Code'])\n",
    "\n",
    "# For HPSA defined by county subdivisions, merge with county subdivision to census tract crosswalk\n",
    "hpsa_csd_with_ct = hpsa_csd.merge(csd_to_ct_2010, left_on='HPSA Geography Identification Number', right_on='csd', how='inner')\n",
    "hpsa_ct_copy = hpsa_ct[['HPSA Geography Identification Number']].rename(columns={'HPSA Geography Identification Number': 'HPSA Geography Identification Number_ct'})\n",
    "merge_csd_ct = hpsa_csd_with_ct.merge(hpsa_ct_copy, left_on='ct', right_on='HPSA Geography Identification Number_ct', how='left', indicator=True)\n",
    "\n",
    "# Drop records where the crosswalked census tract is already in the census tract dataframe\n",
    "merge_csd_ct = merge_csd_ct[(merge_csd_ct['_merge'] != 'both')]\n",
    "# Update 'HPSA Geography Identification Number' to CT\n",
    "merge_csd_ct['HPSA Geography Identification Number'] = merge_csd_ct['ct']\n",
    "\n",
    "# Remove columns that aren't needed\n",
    "merge_csd_ct.drop(columns=['county', 'csd', 'ct', 'HPSA Geography Identification Number_ct', '_merge'], inplace=True)\n",
    "\n",
    "# When the census tract is duplicated, take the mean of the scores.\n",
    "merge_csd_ct_uniques = merge_csd_ct.drop_duplicates(subset='HPSA Geography Identification Number', keep=False, ignore_index=True)\n",
    "merge_csd_ct_duplicates = merge_csd_ct[merge_csd_ct.duplicated(subset='HPSA Geography Identification Number', keep=False)].groupby(['HPSA Geography Identification Number']).agg({'PC_HPSA_Score': 'mean', 'DH_HPSA_Score': 'mean', 'PC MCTA Score': 'mean'})\n",
    "merge_csd_ct_duplicates.reset_index(inplace=True)\n",
    "merge_csd_ct = pd.concat([merge_csd_ct_uniques, merge_csd_ct_duplicates], ignore_index=True)\n",
    "\n",
    "# For HPSA defined by county subdivisions, merge with county subdivision to census tract crosswalk\n",
    "hpsa_cty_with_ct = hpsa_cty.merge(csd_to_ct_2010, left_on='HPSA Geography Identification Number', right_on='county', how='inner')\n",
    "merge_csd_ct_copy = merge_csd_ct[['HPSA Geography Identification Number']].rename(columns={'HPSA Geography Identification Number': 'HPSA Geography Identification Number_ct'})\n",
    "merge_cty_ct = hpsa_cty_with_ct.merge(merge_csd_ct_copy, left_on='ct', right_on='HPSA Geography Identification Number_ct', how='left', indicator=True)\n",
    "\n",
    "# Drop records where the crosswalked census tract is already in the census tract dataframe\n",
    "merge_cty_ct = merge_cty_ct[(merge_cty_ct['_merge'] != 'both')]\n",
    "# Update 'HPSA Geography Identification Number' to census tract\n",
    "merge_cty_ct['HPSA Geography Identification Number'] = merge_cty_ct['ct']\n",
    "\n",
    "# Remove columns that aren't needed\n",
    "merge_cty_ct.drop(columns=['county', 'csd', 'ct', 'HPSA Geography Identification Number_ct', '_merge'], inplace=True)\n",
    "\n",
    "# When the census tract is duplicated, take the mean of the scores.\n",
    "merge_cty_ct_uniques = merge_cty_ct.drop_duplicates(subset='HPSA Geography Identification Number', keep=False, ignore_index=True)\n",
    "merge_cty_ct_duplicates = merge_cty_ct[merge_cty_ct.duplicated(subset='HPSA Geography Identification Number', keep=False)].groupby(['HPSA Geography Identification Number']).agg({'PC_HPSA_Score': 'mean', 'DH_HPSA_Score': 'mean', 'PC MCTA Score': 'mean'}) # When the CT is duplicated, take the mean of the other scores.\n",
    "merge_cty_ct_duplicates.reset_index(inplace=True)\n",
    "merge_cty_ct = pd.concat([merge_cty_ct_uniques, merge_cty_ct_duplicates], ignore_index=True)\n",
    "\n",
    "\n",
    "merge_all_hpsa = pd.concat([hpsa_ct, merge_csd_ct, merge_cty_ct])\n",
    "# merge_all_hpsa['HPSA Geography Identification Number'] = merge_all_hpsa['HPSA Geography Identification Number']\n",
    "\n",
    "# Merge all data together by census tract\n",
    "merge_all_data = pd.merge(ct_500_cities.loc[:, ['MeasureId', 'TractFIPS', 'Data_Value']], merge_all_hpsa, left_on=['TractFIPS'], right_on=['HPSA Geography Identification Number'], how='left')\n",
    "merge_all_data = pd.merge(merge_all_data, acs_dp05, left_on=['TractFIPS'], right_on=['Geography'], how='left')\n",
    "merge_all_data = pd.merge(merge_all_data, acs_s1810, left_on=['TractFIPS'], right_on=['Geography'], how='left')\n",
    "merge_all_data = pd.merge(merge_all_data, acs_dp04, left_on=['TractFIPS'], right_on=['Geography'], how='left')\n",
    "merge_all_data = pd.merge(merge_all_data, acs_dp03, left_on=['TractFIPS'], right_on=['Geography'], how='left')\n",
    "merge_all_data = pd.merge(merge_all_data, acs_dp02, left_on=['TractFIPS'], right_on=['Geography'], how='left')\n",
    "merge_all_data[['PC_HPSA_Score', 'DH_HPSA_Score', 'PC MCTA Score']] = merge_all_data[['PC_HPSA_Score', 'DH_HPSA_Score', 'PC MCTA Score']].fillna(0) # If NaN, these scores weren't on files. This means they aren't considered an HPSA. So a score of 0.\n",
    "\n",
    "# Drop columns that aren't needed anymore\n",
    "merge_all_data = merge_all_data.drop(columns=['TractFIPS', 'HPSA Geography Identification Number', 'cousubfp', 'tract', 'Geography_x', 'Geography_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483\n",
      "483\n"
     ]
    }
   ],
   "source": [
    "print(len(merge_all_data.columns))\n",
    "\n",
    "# columns_to_remove = [col for col in merge_all_data.columns if col.endswith('s')]\n",
    "# columns_to_remove = columns_to_remove + [col for col in merge_all_data.columns if col.endswith('y')]\n",
    "# columns_to_remove = columns_to_remove + [col for col in merge_all_data.columns if col.endswith('r')]\n",
    "# columns_to_remove = columns_to_remove + [col for col in merge_all_data.columns if col.endswith('t')]\n",
    "# columns_to_remove = columns_to_remove + [col for col in merge_all_data.columns if col.endswith('0')]\n",
    "# columns_to_remove = columns_to_remove + [col for col in merge_all_data.columns if col.endswith('9')]\n",
    "# columns_to_remove = columns_to_remove + [col for col in merge_all_data.columns if col.endswith('e') and col != 'Data_Value']\n",
    "\n",
    "ARTH_TRAIN = merge_all_data#.drop(columns=columns_to_remove)                                     \n",
    "\n",
    "print(len(ARTH_TRAIN.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Begin Outcome: STROKE\n",
      "Outcome: STROKE\n",
      "new feature_count 480\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.7368396263977087, Average Test Score: 0.7904446894524072\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.83       983\n",
      "           1       0.68      0.71      0.70       863\n",
      "           2       0.87      0.83      0.85       875\n",
      "\n",
      "    accuracy                           0.79      2721\n",
      "   macro avg       0.79      0.79      0.79      2721\n",
      "weighted avg       0.79      0.79      0.79      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 358\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.7368273731644436, Average Test Score: 0.7891951488423372\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.83       983\n",
      "           1       0.67      0.69      0.68       863\n",
      "           2       0.85      0.83      0.84       875\n",
      "\n",
      "    accuracy                           0.78      2721\n",
      "   macro avg       0.78      0.78      0.78      2721\n",
      "weighted avg       0.79      0.78      0.78      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 267\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.73826884369252, Average Test Score: 0.7903711870635796\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.83       983\n",
      "           1       0.65      0.71      0.68       863\n",
      "           2       0.85      0.80      0.82       875\n",
      "\n",
      "    accuracy                           0.78      2721\n",
      "   macro avg       0.78      0.77      0.78      2721\n",
      "weighted avg       0.78      0.78      0.78      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 199\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.7392406480813871, Average Test Score: 0.7899669239250275\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.81      0.83       983\n",
      "           1       0.68      0.71      0.70       863\n",
      "           2       0.85      0.85      0.85       875\n",
      "\n",
      "    accuracy                           0.79      2721\n",
      "   macro avg       0.79      0.79      0.79      2721\n",
      "weighted avg       0.80      0.79      0.80      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 149\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.739199821759024, Average Test Score: 0.7902241822859243\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.83      0.84       983\n",
      "           1       0.68      0.72      0.70       863\n",
      "           2       0.87      0.83      0.85       875\n",
      "\n",
      "    accuracy                           0.79      2721\n",
      "   macro avg       0.80      0.79      0.79      2721\n",
      "weighted avg       0.80      0.79      0.80      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 111\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.7396653212152373, Average Test Score: 0.7892686512311651\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.82       983\n",
      "           1       0.66      0.69      0.68       863\n",
      "           2       0.86      0.83      0.84       875\n",
      "\n",
      "    accuracy                           0.78      2721\n",
      "   macro avg       0.78      0.78      0.78      2721\n",
      "weighted avg       0.78      0.78      0.78      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 83\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.7405718245010132, Average Test Score: 0.7913267181183389\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82       983\n",
      "           1       0.68      0.71      0.69       863\n",
      "           2       0.87      0.84      0.85       875\n",
      "\n",
      "    accuracy                           0.79      2721\n",
      "   macro avg       0.79      0.79      0.79      2721\n",
      "weighted avg       0.79      0.79      0.79      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 62\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.7415762736045678, Average Test Score: 0.7900404263138552\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.85       983\n",
      "           1       0.70      0.73      0.72       863\n",
      "           2       0.87      0.83      0.85       875\n",
      "\n",
      "    accuracy                           0.81      2721\n",
      "   macro avg       0.81      0.81      0.81      2721\n",
      "weighted avg       0.81      0.81      0.81      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 46\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.7419438255753287, Average Test Score: 0.7894524072032342\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.84      0.84       983\n",
      "           1       0.67      0.72      0.69       863\n",
      "           2       0.88      0.80      0.84       875\n",
      "\n",
      "    accuracy                           0.79      2721\n",
      "   macro avg       0.79      0.79      0.79      2721\n",
      "weighted avg       0.79      0.79      0.79      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 34\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.7409310606163572, Average Test Score: 0.7871003307607498\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.84      0.82       983\n",
      "           1       0.67      0.68      0.67       863\n",
      "           2       0.87      0.82      0.85       875\n",
      "\n",
      "    accuracy                           0.78      2721\n",
      "   macro avg       0.78      0.78      0.78      2721\n",
      "weighted avg       0.78      0.78      0.78      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 25\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.7388566153242713, Average Test Score: 0.7834252113193679\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.82       983\n",
      "           1       0.67      0.69      0.68       863\n",
      "           2       0.86      0.82      0.84       875\n",
      "\n",
      "    accuracy                           0.78      2721\n",
      "   macro avg       0.78      0.78      0.78      2721\n",
      "weighted avg       0.78      0.78      0.78      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 24\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.7380154139754234, Average Test Score: 0.7822124219037119\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83       983\n",
      "           1       0.66      0.68      0.67       863\n",
      "           2       0.85      0.81      0.83       875\n",
      "\n",
      "    accuracy                           0.78      2721\n",
      "   macro avg       0.78      0.78      0.78      2721\n",
      "weighted avg       0.78      0.78      0.78      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 23\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.7378397678998307, Average Test Score: 0.7807056229327453\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.83       983\n",
      "           1       0.66      0.69      0.68       863\n",
      "           2       0.86      0.82      0.84       875\n",
      "\n",
      "    accuracy                           0.78      2721\n",
      "   macro avg       0.78      0.78      0.78      2721\n",
      "weighted avg       0.78      0.78      0.78      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 22\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.7365453461769789, Average Test Score: 0.7793825799338479\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81       983\n",
      "           1       0.66      0.69      0.68       863\n",
      "           2       0.86      0.83      0.84       875\n",
      "\n",
      "    accuracy                           0.78      2721\n",
      "   macro avg       0.78      0.78      0.78      2721\n",
      "weighted avg       0.78      0.78      0.78      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 21\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.7371537644442439, Average Test Score: 0.7785740536567439\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.82       983\n",
      "           1       0.66      0.69      0.67       863\n",
      "           2       0.85      0.82      0.84       875\n",
      "\n",
      "    accuracy                           0.78      2721\n",
      "   macro avg       0.78      0.78      0.78      2721\n",
      "weighted avg       0.78      0.78      0.78      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 20\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.736966061920895, Average Test Score: 0.779566335905917\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.82       983\n",
      "           1       0.67      0.69      0.68       863\n",
      "           2       0.84      0.83      0.84       875\n",
      "\n",
      "    accuracy                           0.78      2721\n",
      "   macro avg       0.78      0.78      0.78      2721\n",
      "weighted avg       0.78      0.78      0.78      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 19\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.7363412854412995, Average Test Score: 0.7781330393237781\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81       983\n",
      "           1       0.65      0.67      0.66       863\n",
      "           2       0.85      0.82      0.83       875\n",
      "\n",
      "    accuracy                           0.77      2721\n",
      "   macro avg       0.77      0.77      0.77      2721\n",
      "weighted avg       0.77      0.77      0.77      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 18\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.7348387363602001, Average Test Score: 0.776111723631018\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83       983\n",
      "           1       0.65      0.68      0.67       863\n",
      "           2       0.86      0.80      0.83       875\n",
      "\n",
      "    accuracy                           0.78      2721\n",
      "   macro avg       0.78      0.77      0.78      2721\n",
      "weighted avg       0.78      0.78      0.78      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 17\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.731400588338641, Average Test Score: 0.7719955898566704\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82       983\n",
      "           1       0.64      0.66      0.65       863\n",
      "           2       0.83      0.80      0.81       875\n",
      "\n",
      "    accuracy                           0.76      2721\n",
      "   macro avg       0.76      0.76      0.76      2721\n",
      "weighted avg       0.77      0.76      0.76      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 16\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.7326296589281877, Average Test Score: 0.7699007717750826\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.81      0.82       983\n",
      "           1       0.64      0.67      0.66       863\n",
      "           2       0.84      0.81      0.82       875\n",
      "\n",
      "    accuracy                           0.77      2721\n",
      "   macro avg       0.77      0.76      0.76      2721\n",
      "weighted avg       0.77      0.77      0.77      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 15\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.7327726919748951, Average Test Score: 0.7692024990812202\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.81       983\n",
      "           1       0.65      0.65      0.65       863\n",
      "           2       0.86      0.83      0.85       875\n",
      "\n",
      "    accuracy                           0.77      2721\n",
      "   macro avg       0.77      0.77      0.77      2721\n",
      "weighted avg       0.77      0.77      0.77      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 14\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.7334382255683912, Average Test Score: 0.7699007717750826\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83       983\n",
      "           1       0.67      0.70      0.68       863\n",
      "           2       0.85      0.82      0.83       875\n",
      "\n",
      "    accuracy                           0.78      2721\n",
      "   macro avg       0.78      0.78      0.78      2721\n",
      "weighted avg       0.79      0.78      0.78      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 13\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.7318947734519338, Average Test Score: 0.770011025358324\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.84       983\n",
      "           1       0.67      0.70      0.68       863\n",
      "           2       0.86      0.79      0.82       875\n",
      "\n",
      "    accuracy                           0.78      2721\n",
      "   macro avg       0.78      0.78      0.78      2721\n",
      "weighted avg       0.78      0.78      0.78      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 12\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.7314700936473886, Average Test Score: 0.7684307239985299\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.82       983\n",
      "           1       0.66      0.69      0.68       863\n",
      "           2       0.86      0.82      0.84       875\n",
      "\n",
      "    accuracy                           0.78      2721\n",
      "   macro avg       0.78      0.78      0.78      2721\n",
      "weighted avg       0.78      0.78      0.78      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 11\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.7277172639176344, Average Test Score: 0.7654906284454244\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.80       983\n",
      "           1       0.64      0.68      0.66       863\n",
      "           2       0.86      0.81      0.84       875\n",
      "\n",
      "    accuracy                           0.77      2721\n",
      "   macro avg       0.77      0.76      0.77      2721\n",
      "weighted avg       0.77      0.77      0.77      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "new feature_count 10\n",
      "Beginning Model Training\n",
      "\n",
      "\n",
      "End of Split: 1\n",
      "End of Split: 2\n",
      "End of Split: 3\n",
      "End of Split: 4\n",
      "End of Split: 5\n",
      "End of Split: 6\n",
      "End of Split: 7\n",
      "End of Split: 8\n",
      "End of Split: 9\n",
      "End of Split: 10\n",
      "Average Train Score: 0.7217349877880417, Average Test Score: 0.7589121646453509\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       983\n",
      "           1       0.64      0.63      0.64       863\n",
      "           2       0.82      0.81      0.82       875\n",
      "\n",
      "    accuracy                           0.76      2721\n",
      "   macro avg       0.76      0.76      0.76      2721\n",
      "weighted avg       0.76      0.76      0.76      2721\n",
      "\n",
      "Outcome: STROKE\n",
      "\n",
      "Final Permutation Feature Importances:\n",
      "Percent Estimate!!GROSS RENT!!Occupied units paying rent!!Less than $500: 0.021087835354649025\n",
      "Percent Estimate!!GROSS RENT!!Occupied units paying rent!!$500 to $999: 0.02177140757074604\n",
      "Percent Estimate!!HISPANIC OR LATINO AND RACE!!Total population!!Not Hispanic or Latino!!Black or African American alone: 0.026196251378169787\n",
      "Percent Estimate!!Race alone or in combination with one or more other races!!Total population!!Black or African American: 0.026784270488790896\n",
      "Percent Estimate!!GROSS RENT AS A PERCENTAGE OF HOUSEHOLD INCOME (GRAPI)!!Occupied units paying rent (excluding units where GRAPI cannot be computed)!!35.0 percent or more: 0.027335538404998166\n",
      "Percent Estimate!!EMPLOYMENT STATUS!!Population 16 years and over!!In labor force!!Civilian labor force!!Employed: 0.02780595369349504\n",
      "Percent Estimate!!VALUE!!Owner-occupied units!!$50,000 to $99,999: 0.031223814773980157\n",
      "Percent Estimate!!EDUCATIONAL ATTAINMENT!!Population 25 years and over!!Percent bachelor's degree or higher: 0.05337743476662991\n",
      "Percent Estimate!!SEX AND AGE!!Total population!!65 years and over: 0.05711870635795664\n",
      "Percent Estimate!!HEALTH INSURANCE COVERAGE!!Civilian noninstitutionalized population!!With health insurance coverage!!With public coverage: 0.08002940095553104\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier as ensemble\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report\n",
    "import shap\n",
    "\n",
    "measure_id_set = set(ct_500_cities['MeasureId'])\n",
    "\n",
    "for measure in measure_id_set:\n",
    "\n",
    "  # Get data for only the selected measure, and drop any census tracts where there is a NaN Data_Value for that meausre.\n",
    "  merge_data_measure = ARTH_TRAIN[merge_all_data['MeasureId'] == measure].dropna(subset=['Data_Value'])\n",
    "  merge_data_measure = merge_data_measure.reset_index(drop=True)\n",
    "  scaler = StandardScaler()\n",
    "\n",
    "  y = merge_data_measure['Data_Value']\n",
    "\n",
    "  least_important = ['start']\n",
    "  test_scores = [1]\n",
    "  feature_count = len(ARTH_TRAIN.columns) - 1\n",
    "  \n",
    "  if measure == 'ACCESS2': # Remove health insurance coverage features since that is what we're predicting with the measure\n",
    "     merge_data_measure = merge_data_measure.drop(columns=['Percent Estimate!!HEALTH INSURANCE COVERAGE!!Civilian noninstitutionalized population!!No health insurance coverage', 'Percent Estimate!!HEALTH INSURANCE COVERAGE!!Civilian noninstitutionalized population!!Civilian noninstitutionalized population 19 to 64 years!!Not in labor force!!No health insurance coverage',\n",
    "                                                           'Percent Estimate!!HEALTH INSURANCE COVERAGE!!Civilian noninstitutionalized population!!Civilian noninstitutionalized population 19 to 64 years!!In labor force!!Unemployed!!No health insurance coverage', 'Percent Estimate!!HEALTH INSURANCE COVERAGE!!Civilian noninstitutionalized population!!Civilian noninstitutionalized population under 19 years!!No health insurance coverage'])\n",
    "\n",
    "  print(f\"\\n\\nBegin Outcome: {measure}\")\n",
    "\n",
    "  # while len(least_important) >= 1 and ((feature_count - len(least_important)) >= 10 or np.mean(test_scores) > 0.7):\n",
    "  # Keep running until there are only 10 features left, or the mean test score is below 75%\n",
    "  while feature_count > 10 and np.mean(test_scores) > 0.75:\n",
    "    print(f\"Outcome: {measure}\")\n",
    "\n",
    "    # Recursive Feature Elimination - Drop the features that were the least important from the previous iteration\n",
    "    if least_important[0] != 'start':\n",
    "      merge_data_measure = merge_data_measure.drop(columns=least_important)\n",
    "\n",
    "    # Keep only the numeric features. Drop data_value column since that will be the predicted variable.\n",
    "    X_numeric = merge_data_measure.select_dtypes(include='number').drop(columns=['Data_Value'])\n",
    "    # Scale the features\n",
    "    X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "    # Create 3 bins for the outcomes data. All 3 will be of equal size. The cutoff for each quantile is different for each measure.\n",
    "    y_binned = pd.qcut(y, q=3, labels=False)\n",
    "\n",
    "    feature_count = len(X_numeric.columns)\n",
    "    print(\"new feature_count\", feature_count)\n",
    "\n",
    "    # Initiate StratifiedKFold\n",
    "    N_SPLITS = 10\n",
    "    SKF = StratifiedKFold(n_splits=N_SPLITS, shuffle=True)\n",
    "    SKF.get_n_splits(X = X_numeric_scaled, y = y_binned)\n",
    "\n",
    "    # Instructor says max depth 1-5 for random forests\n",
    "    # StatQuest says max_leaf_nodes is usually 8-32\n",
    "    # parameters = {'class_weight': [None], 'early_stopping': ['auto'], 'interaction_cst': [None], 'l2_regularization': [0.0], 'learning_rate': [0.1],\n",
    "    #               'loss': ['log_loss'],'max_bins': [10, 20, 100], 'max_depth': [5], 'max_iter': [25, 50, 100], 'max_leaf_nodes': [8, 16, 32],\n",
    "    #               'min_samples_leaf': [10, 100, 500], 'monotonic_cst': [None], 'n_iter_no_change': [10], 'random_state': [None], 'scoring': ['loss'],\n",
    "    #               'tol': [1e-07], 'validation_fraction': [0.1], 'verbose': [0], 'warm_start': [False]}\n",
    "    parameters = {'class_weight': [None], 'early_stopping': ['auto'], 'interaction_cst': [None], 'l2_regularization': [0.0], 'learning_rate': [0.1], 'loss': ['log_loss'],'max_bins': [10],\n",
    "                  'max_depth': [5], 'max_iter': [50], 'max_leaf_nodes': [16], 'min_samples_leaf': [100], 'monotonic_cst': [None], 'n_iter_no_change': [10], 'random_state': [None],\n",
    "                  'scoring': ['loss'], 'tol': [1e-07], 'validation_fraction': [0.1], 'verbose': [0], 'warm_start': [False]}\n",
    "\n",
    "    param_combo_scores = {}\n",
    "    param_combo_times = {}\n",
    "    test_scores = []\n",
    "    train_scores = []\n",
    "    feature_importances_all = []\n",
    "\n",
    "    scoring = 'accuracy'\n",
    "\n",
    "    # Initiate a Ensemble classifier\n",
    "    clf = ensemble()\n",
    "\n",
    "    current_best = 0\n",
    "    print(\"Beginning Model Training\\n\\n\")\n",
    "\n",
    "    # Train on each split\n",
    "    for index, (train_index, test_index) in enumerate(SKF.split(X = X_numeric_scaled, y = y_binned)):\n",
    "        x_train = np.take(X_numeric_scaled, train_index, 0)\n",
    "        y_train = np.take(y_binned, train_index, 0)\n",
    "\n",
    "        x_test = np.take(X_numeric_scaled, test_index, 0)\n",
    "        y_test = np.take(y_binned, test_index, 0)\n",
    "\n",
    "        grid = GridSearchCV(clf, param_grid = parameters, scoring = scoring)\n",
    "        grid.fit(X = x_train, y = y_train)\n",
    "        best_estimator = grid.best_estimator_\n",
    "        test_score = grid.score(X = x_test, y = y_test)\n",
    "\n",
    "        feature_importances_all.append(permutation_importance(grid.best_estimator_, x_test, y_test))\n",
    "        y_pred = best_estimator.predict(x_test)\n",
    "\n",
    "        for params, score, time in zip(grid.cv_results_['params'], grid.cv_results_['mean_test_score'], grid.cv_results_['mean_fit_time']):\n",
    "          param_str = str(params)\n",
    "          if param_str not in param_combo_scores:\n",
    "              param_combo_scores[param_str] = []\n",
    "              param_combo_times[param_str] = []\n",
    "          param_combo_scores[param_str].append(score)\n",
    "          param_combo_times[param_str].append(time)\n",
    "\n",
    "        test_scores.append(test_score)\n",
    "        train_scores.append(grid.best_score_)\n",
    "\n",
    "        print(f\"End of Split: {index + 1}\")\n",
    "        \n",
    "    print(f\"Average Train Score: {np.mean(train_scores)}, Average Test Score: {np.mean(test_scores)}\")\n",
    "\n",
    "    # # Compute average score and time for each parameter combination\n",
    "    # avg_scores = {param: np.mean(scores) for param, scores in param_combo_scores.items()}\n",
    "    # avg_times = {param: np.mean(times) for param, times in param_combo_times.items()}\n",
    "\n",
    "    # # Print average scores and times for each parameter combination\n",
    "    # for param, avg_score in avg_scores.items():\n",
    "    #     avg_time = avg_times[param]\n",
    "    #     print(f\"Parameter Combo: {param}, Average Score: {avg_score}, Average Time: {avg_time}\")\n",
    "    # break\n",
    "\n",
    "    print(\"Classification Report:\", classification_report(y_test, y_pred))\n",
    "    joblib.dump(grid, f\"Models\\{measure}_model_{feature_count}.pkl\")\n",
    "\n",
    "    # feature_importances = permutation_importance(grid.best_estimator_, x_test, y_test)\n",
    "    # feature_importances_index = feature_importances.importances_mean.argsort()\n",
    "\n",
    "    # Average feature importance across all splits.\n",
    "    feature_importances = np.mean([fi.importances_mean for fi in feature_importances_all], axis=0)\n",
    "    feature_importances_index = feature_importances.argsort()\n",
    "\n",
    "    least_important = []\n",
    "\n",
    "    distinct_importances = list(set(abs(feature_importances)))\n",
    "\n",
    "    remove_value = 0\n",
    "\n",
    "    # Getting a list of the least important features for removal.\n",
    "    # If there are more than 25 features left, loop until our list of removal features is great than 25% of the remaining features. If there are 25 or less, remove 1 feature at a time.\n",
    "    while (len(least_important) < feature_count*0.25 and (feature_count - len(least_important)) > 25) or len(least_important) == 0:\n",
    "      for i in feature_importances_index:\n",
    "          if abs(feature_importances[i]) == remove_value:\n",
    "              least_important.append(X_numeric.columns[i])\n",
    "      if remove_value in distinct_importances:\n",
    "        distinct_importances.remove(remove_value)\n",
    "      if -remove_value in distinct_importances:\n",
    "        distinct_importances.remove(-remove_value)\n",
    "      if len(distinct_importances) > 0:\n",
    "        remove_value = abs(min(distinct_importances))    \n",
    "  \n",
    "  print(f\"Outcome: {measure}\")\n",
    "  print(\"\\nFinal Permutation Feature Importances:\")\n",
    "  for i in feature_importances_index:\n",
    "      print(f\"{X_numeric.columns[i]}: {feature_importances[i]}\")\n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = ARTH_TRAIN.corr()\n",
    "\n",
    "upper_triangle = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "upper_triangle = corr_matrix.where(upper_triangle)\n",
    "upper_triangle.to_csv('upper_correlation_matrix.csv', index=True, header=True)\n",
    "corr_matrix.to_csv('correlation_matrix.csv', index=True, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
